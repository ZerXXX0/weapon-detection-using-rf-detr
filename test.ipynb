{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f9fad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RF-DETR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_classes mismatch: pretrain weights has 1 classes, but your model has 90 classes\n",
      "reinitializing detection head with 1 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrain weights\n",
      "Model loaded.\n",
      "Loading ground truth annotations from: 1-class-weapon-dataset-1/test/_annotations.coco.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Ground truth loaded.\n",
      "Processing images from folder: 1-class-weapon-dataset-1/test\n",
      "Finished processing images.\n",
      "Saving 533 predictions to rf_detr_predictions_1class.json...\n",
      "Predictions saved.\n",
      "\n",
      "--- Running COCO Evaluation ---\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "\n",
      "COCO Evaluation Summary:\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.233\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.076\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.166\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.106\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.110\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.450\n",
      "--- Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from rfdetr import RFDETRBase # Or other specific RF-DETR model class\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# --- Configuration ---\n",
    "IMAGE_FOLDER = \"1-class-weapon-dataset-1/test\" # !!! CHANGE THIS !!!\n",
    "GROUND_TRUTH_ANNOTATION_FILE = \"1-class-weapon-dataset-1/test/_annotations.coco.json\" # !!! CHANGE THIS !!! Needed for evaluation\n",
    "OUTPUT_PREDICTIONS_FILE = \"rf_detr_predictions_1class.json\" # Optional: Save predictions\n",
    "\n",
    "# --- Load RF-DETR Model ---\n",
    "# Load the base model (or specify weights if you have a fine-tuned model)\n",
    "print(\"Loading RF-DETR model...\")\n",
    "model = RFDETRBase(pretrain_weights='output3/checkpoint_best_ema.pth')\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Prepare for Predictions & Evaluation ---\n",
    "all_predictions_coco_format = []\n",
    "image_id_map = {} # Needed if your annotations use COCO image IDs\n",
    "\n",
    "# --- Load Ground Truth (Needed for Evaluation) ---\n",
    "# This assumes your annotations are in COCO format. Adjust if necessary.\n",
    "try:\n",
    "    print(f\"Loading ground truth annotations from: {GROUND_TRUTH_ANNOTATION_FILE}\")\n",
    "    cocoGt = COCO(GROUND_TRUTH_ANNOTATION_FILE)\n",
    "\n",
    "    # Create a mapping from filename to COCO image ID (if needed)\n",
    "    for img_info in cocoGt.dataset['images']:\n",
    "        image_id_map[img_info['file_name']] = img_info['id']\n",
    "    print(\"Ground truth loaded.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Ground truth annotation file not found at {GROUND_TRUTH_ANNOTATION_FILE}\")\n",
    "    print(\"Evaluation cannot be performed without ground truth.\")\n",
    "    cocoGt = None\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading ground truth annotations: {e}\")\n",
    "    cocoGt = None\n",
    "\n",
    "\n",
    "# --- Iterate Through Images and Predict ---\n",
    "print(f\"Processing images from folder: {IMAGE_FOLDER}\")\n",
    "for filename in os.listdir(IMAGE_FOLDER):\n",
    "    # Check if it's an image file (add more extensions if needed)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "        image_path = os.path.join(IMAGE_FOLDER, filename)\n",
    "        # print(f\"  Predicting on: {filename}\")\n",
    "\n",
    "        try:\n",
    "            # Load Image\n",
    "            image = Image.open(image_path).convert(\"RGB\") # Ensure image is RGB\n",
    "            image_width, image_height = image.size\n",
    "\n",
    "            # Perform Inference\n",
    "            # Note: The 'predict' method might return detections in a specific format.\n",
    "            # You'll need to inspect the 'detections' object structure.\n",
    "            # Common attributes might be .xyxy, .confidence, .class_id\n",
    "            detections = model.predict(image, threshold=0.5) # Adjust threshold as needed\n",
    "\n",
    "            # --- Format Predictions for COCOeval ---\n",
    "            # This part requires knowing the exact structure of 'detections'\n",
    "            # and having the 'image_id' from your ground truth annotations.\n",
    "            if cocoGt and filename in image_id_map:\n",
    "                current_image_id = image_id_map[filename]\n",
    "\n",
    "                # Example loop - **ADAPT BASED ON `detections` STRUCTURE**\n",
    "                for i in range(len(detections)): # Assuming detections is indexable\n",
    "                   # Example accessors - REPLACE with actual attributes/methods\n",
    "                   # Make sure bbox format is [x, y, width, height]\n",
    "                   box = detections.xyxy[i] # Assuming xyxy format [xmin, ymin, xmax, ymax]\n",
    "                   score = detections.confidence[i] # Assuming confidence attribute\n",
    "                   class_id = detections.class_id[i] # Assuming class_id attribute\n",
    "\n",
    "                   # Convert bbox from [xmin, ymin, xmax, ymax] to [x, y, width, height]\n",
    "                   x = float(box[0])\n",
    "                   y = float(box[1])\n",
    "                   width = float(box[2] - box[0])\n",
    "                   height = float(box[3] - box[1])\n",
    "                   coco_bbox = [x, y, width, height]\n",
    "\n",
    "                   # Get the category ID that matches your ground truth file's categories\n",
    "                   # This might require a mapping if your model outputs names or different IDs\n",
    "                   # For simplicity, assuming model class_id directly maps to COCO category_id\n",
    "                   category_id = int(class_id) # Adapt if needed\n",
    "\n",
    "                   prediction_entry = {\n",
    "                       \"image_id\": current_image_id,\n",
    "                       \"category_id\": category_id, # Ensure this matches GT category IDs\n",
    "                       \"bbox\": coco_bbox,\n",
    "                       \"score\": float(score)\n",
    "                   }\n",
    "                   all_predictions_coco_format.append(prediction_entry)\n",
    "            else:\n",
    "                 if not cocoGt:\n",
    "                     print(f\"  Skipping prediction formatting for {filename} (Ground truth not loaded)\")\n",
    "                 else:\n",
    "                     print(f\"  Skipping prediction formatting for {filename} (Filename not found in ground truth image map)\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {filename}: {e}\")\n",
    "\n",
    "print(\"Finished processing images.\")\n",
    "\n",
    "# --- Save Predictions (Optional) ---\n",
    "if all_predictions_coco_format:\n",
    "    print(f\"Saving {len(all_predictions_coco_format)} predictions to {OUTPUT_PREDICTIONS_FILE}...\")\n",
    "    with open(OUTPUT_PREDICTIONS_FILE, 'w') as f:\n",
    "        json.dump(all_predictions_coco_format, f, indent=4)\n",
    "    print(\"Predictions saved.\")\n",
    "\n",
    "# --- Calculate Evaluation Metrics (Requires Ground Truth and Predictions) ---\n",
    "if cocoGt and all_predictions_coco_format:\n",
    "    print(\"\\n--- Running COCO Evaluation ---\")\n",
    "    # Load predictions into COCO format\n",
    "    cocoDt = cocoGt.loadRes(all_predictions_coco_format)\n",
    "\n",
    "    # Initialize evaluator\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, iouType='bbox') # Use 'bbox' for detection\n",
    "\n",
    "    # Run evaluation\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nCOCO Evaluation Summary:\")\n",
    "    cocoEval.summarize()\n",
    "    print(\"--- Evaluation Complete ---\")\n",
    "\n",
    "elif not cocoGt:\n",
    "    print(\"\\nEvaluation skipped because ground truth annotations could not be loaded.\")\n",
    "else:\n",
    "    print(\"\\nEvaluation skipped because no predictions were successfully formatted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84671ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rfdetr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
